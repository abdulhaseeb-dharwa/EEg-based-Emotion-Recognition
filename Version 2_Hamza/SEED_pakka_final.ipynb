{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-12-09 02:18:38] INFO (torcheeg/MainThread) üîç | Detected cached processing results, reading cache from E:/FYP/Egg-Based Emotion Recognition/EEg-based-Emotion-Recognition/.torcheeg/datasets_1733174610032_5iJyS.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw EEG data shape: (62, 200)\n",
      "Label: {'start_at': 0, 'end_at': 200, 'clip_id': '10_20131130.mat_0', 'subject_id': 10, 'trial_id': 'ww_eeg1', 'emotion': 1, 'date': 20131130, '_record_id': '_record_0'}\n"
     ]
    }
   ],
   "source": [
    "from torcheeg.datasets import SEEDDataset\n",
    "from torcheeg import transforms\n",
    "\n",
    "raw_dataset = SEEDDataset(\n",
    "    root_path='./SEED/SEED_EEG/Preprocessed_EEG',\n",
    "    io_path = 'E:/FYP/Egg-Based Emotion Recognition/EEg-based-Emotion-Recognition/.torcheeg/datasets_1733174610032_5iJyS',\n",
    "    online_transform=None,  # Disable transforms\n",
    "    label_transform=None,\n",
    "    num_worker=4\n",
    ")\n",
    "\n",
    "raw_sample = raw_dataset[0]\n",
    "print(f\"Raw EEG data shape: {raw_sample[0].shape}\")  # Should be [62, ...] for SEED\n",
    "print(f\"Label: {raw_sample[1]}\")  # Should be a number between 0 and 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.signal import butter, lfilter , filtfilt\n",
    "\n",
    "#Bandpass filter function\n",
    "def bandpass_filter(data, lowcut=4, highcut=47, fs=200, order=5):\n",
    "    nyquist = 0.5 * fs\n",
    "    low = lowcut / nyquist\n",
    "    high = highcut / nyquist\n",
    "    b, a = butter(order, [low, high], btype='band')\n",
    "    y = filtfilt(b, a, data, axis=1)\n",
    "    return y\n",
    "\n",
    "# Stratified normalization function\n",
    "def stratified_normalization(data):\n",
    "    mean = np.mean(data, axis=1, keepdims=True)\n",
    "    std = np.std(data, axis=1, keepdims=True)\n",
    "    return (data - mean) / (std + 1e-8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed EEG data shape: (152730, 62, 200)\n",
      "Sample label: {'start_at': 0, 'end_at': 200, 'clip_id': '10_20131130.mat_0', 'subject_id': 10, 'trial_id': 'ww_eeg1', 'emotion': 1, 'date': 20131130, '_record_id': '_record_0'}\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the EEG data\n",
    "preprocessed_data = []\n",
    "labels = []\n",
    "\n",
    "for i in range(len(raw_dataset)):\n",
    "    eeg_data, label = raw_dataset[i]\n",
    "    eeg_data = np.array(eeg_data)  # Convert to numpy array if needed\n",
    "\n",
    "    # Step 1: Bandpass filtering\n",
    "    eeg_data_filtered = bandpass_filter(eeg_data)\n",
    "\n",
    "    # Step 2: Stratified normalization\n",
    "    eeg_data_normalized = stratified_normalization(eeg_data_filtered)\n",
    "\n",
    "    preprocessed_data.append(eeg_data_normalized)\n",
    "    labels.append(label)\n",
    "\n",
    "# Convert to numpy arrays\n",
    "preprocessed_data = np.array(preprocessed_data)\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Display shape of preprocessed data\n",
    "print(f\"Preprocessed EEG data shape: {preprocessed_data.shape}\")  # Should be (num_samples, 62, 200)\n",
    "print(f\"Sample label: {labels[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([1, 16, 200])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class BaseEncoder(nn.Module):\n",
    "    def __init__(self, num_channels=62, spatial_filters=16, temporal_filters=16, temporal_filter_length=48):\n",
    "        super(BaseEncoder, self).__init__()\n",
    "        \n",
    "        self.spatial_conv = nn.Conv1d(\n",
    "            in_channels=num_channels, \n",
    "            out_channels=spatial_filters, \n",
    "            kernel_size=1\n",
    "        )\n",
    "        \n",
    "        self.temporal_conv = nn.Conv1d(\n",
    "            in_channels=spatial_filters, \n",
    "            out_channels=temporal_filters, \n",
    "            kernel_size=temporal_filter_length, \n",
    "            padding='same'  # Padding to maintain input length\n",
    "        )\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.spatial_conv(x)  # Output shape: [batch_size, spatial_filters, time_points]\n",
    "        x = self.relu(x)\n",
    "        x = self.temporal_conv(x)  # Output shape: [batch_size, temporal_filters, time_points]\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Instantiate the Base Encoder\n",
    "base_encoder = BaseEncoder()\n",
    "\n",
    "# Example input tensor: batch_size=1, num_channels=62, time_points=200\n",
    "example_input = torch.randn(1, 62, 200)\n",
    "\n",
    "# Forward pass\n",
    "output = base_encoder(example_input)\n",
    "print(f\"Output shape: {output.shape}\")  # Expected: [1, 16, 200]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([1, 64, 8])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Projector(nn.Module):\n",
    "    def __init__(self, spatial_filter_size=16, temporal_filter_size=4, avg_pool_kernel=24, c=2):\n",
    "        super(Projector, self).__init__()\n",
    "        \n",
    "        # Average Pooling\n",
    "        self.avg_pool = nn.AvgPool1d(kernel_size=avg_pool_kernel, stride=avg_pool_kernel)\n",
    "        \n",
    "        # Spatial Convolution (Depthwise)\n",
    "        self.spatial_conv = nn.Conv1d(\n",
    "            in_channels=spatial_filter_size, \n",
    "            out_channels=c * spatial_filter_size, \n",
    "            kernel_size=1, \n",
    "            groups=spatial_filter_size  # Depthwise convolution\n",
    "        )\n",
    "        \n",
    "        # Temporal Convolution (Depthwise)\n",
    "        self.temporal_conv = nn.Conv1d(\n",
    "            in_channels=c * spatial_filter_size, \n",
    "            out_channels=c**2 * spatial_filter_size, \n",
    "            kernel_size=temporal_filter_size, \n",
    "            padding='same', \n",
    "            groups=c * spatial_filter_size  # Depthwise convolution\n",
    "        )\n",
    "        \n",
    "        # Activation Function\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.avg_pool(x)  # Output shape: [batch_size, spatial_filter_size, reduced_time_points]\n",
    "        x = self.spatial_conv(x)  # Output shape: [batch_size, c * spatial_filter_size, reduced_time_points]\n",
    "        x = self.relu(x)\n",
    "        x = self.temporal_conv(x)  # Output shape: [batch_size, c^2 * spatial_filter_size, reduced_time_points]\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Instantiate the Projector\n",
    "projector = Projector()\n",
    "\n",
    "# Example input tensor: batch_size=1, spatial_filter_size=16, time_points=200\n",
    "example_input = torch.randn(1, 16, 200)\n",
    "\n",
    "# Forward pass\n",
    "output = projector(example_input)\n",
    "print(f\"Output shape: {output.shape}\")  # Expected: [1, 64, 8] (200 / 24 = ~8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ACTUAL WORKING START"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed Data Shape: (152730, 62, 200)\n",
      "First Label: {'start_at': 0, 'end_at': 200, 'clip_id': '10_20131130.mat_0', 'subject_id': 10, 'trial_id': 'ww_eeg1', 'emotion': 1, 'date': 20131130, '_record_id': '_record_0'}\n",
      "Number of Samples: 152730\n"
     ]
    }
   ],
   "source": [
    "# Check the shape of the preprocessed data\n",
    "print(f\"Preprocessed Data Shape: {preprocessed_data.shape}\")\n",
    "\n",
    "# Check the first few labels\n",
    "print(f\"First Label: {labels[0]}\")\n",
    "print(f\"Number of Samples: {len(preprocessed_data)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Subject: 1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import random\n",
    "\n",
    "class EEGContrastiveDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        eeg_a = self.data[idx]\n",
    "        label_a = self.labels[idx]['emotion']\n",
    "        subject_a = self.labels[idx]['subject_id']\n",
    "\n",
    "        # Find a positive sample (same emotion, different subject)\n",
    "        positive_indices = [i for i, lbl in enumerate(self.labels) if lbl['emotion'] == label_a and lbl['subject_id'] != subject_a]\n",
    "        pos_idx = random.choice(positive_indices)\n",
    "\n",
    "        eeg_b = self.data[pos_idx]\n",
    "        return eeg_a, eeg_b\n",
    "\n",
    "# Extract subject IDs\n",
    "subject_ids = [label['subject_id'] for label in labels]\n",
    "\n",
    "# Initialize LOSO cross-validator\n",
    "logo = LeaveOneGroupOut()\n",
    "\n",
    "# Iterate through each LOSO fold\n",
    "for train_idx, val_idx in logo.split(preprocessed_data, labels, subject_ids):\n",
    "    # Split the data into training and validation\n",
    "    train_data, val_data = preprocessed_data[train_idx], preprocessed_data[val_idx]\n",
    "    train_labels, val_labels = [labels[i] for i in train_idx], [labels[i] for i in val_idx]\n",
    "\n",
    "    # Create DataLoader for training and validation\n",
    "    train_loader = DataLoader(EEGContrastiveDataset(train_data, train_labels), batch_size=16, shuffle=True)\n",
    "    val_loader = DataLoader(EEGContrastiveDataset(val_data, val_labels), batch_size=16, shuffle=False)\n",
    "\n",
    "    # Print the current subject being used for validation\n",
    "    val_subject_id = subject_ids[val_idx[0]]\n",
    "    print(f\"Validation Subject: {val_subject_id}\")\n",
    "    \n",
    "    # Break after the first fold for demonstration purposes\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import random\n",
    "\n",
    "# Combine Base Encoder and Projector into a single model\n",
    "class CLISA(nn.Module):\n",
    "    def __init__(self, base_encoder, projector):\n",
    "        super(CLISA, self).__init__()\n",
    "        self.base_encoder = base_encoder\n",
    "        self.projector = projector\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.base_encoder(x)\n",
    "        x = self.projector(x)\n",
    "        return x\n",
    "\n",
    "# Contrastive Loss Function\n",
    "def contrastive_loss(z_a, z_b, temperature=0.5):\n",
    "    batch_size = z_a.shape[0]\n",
    "    z = torch.cat([z_a, z_b], dim=0)\n",
    "    sim = torch.matmul(z, z.T) / temperature\n",
    "\n",
    "    labels = torch.cat([torch.arange(batch_size), torch.arange(batch_size)], dim=0).to(z.device)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    loss = loss_fn(sim, labels)\n",
    "    return loss\n",
    "\n",
    "# # Data Sampler for Positive and Negative Pairs\n",
    "# class EEGContrastiveDataset(Dataset):\n",
    "#     def __init__(self, data, labels):\n",
    "#         self.data = data\n",
    "#         self.labels = labels\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.data)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         eeg_a = self.data[idx]\n",
    "#         label_a = self.labels[idx]['emotion']\n",
    "#         subject_a = self.labels[idx]['subject_id']\n",
    "\n",
    "#         # Find a positive sample (same emotion, different subject)\n",
    "#         positive_indices = [i for i, lbl in enumerate(self.labels) if lbl['emotion'] == label_a and lbl['subject_id'] != subject_a]\n",
    "#         pos_idx = random.choice(positive_indices)\n",
    "\n",
    "#         eeg_b = self.data[pos_idx]\n",
    "#         return eeg_a, eeg_b\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(model, optimizer, epoch, loss, path):\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss\n",
    "    }, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100]\n",
      "Training...\n",
      "Training Loss:  tensor(3.4427, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(3.5692, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(3.4501, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(3.4936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(3.4544, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(3.4447, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(3.4668, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(3.4884, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(3.3921, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(3.4561, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(3.3979, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(3.3932, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(3.4238, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(3.4174, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(3.4288, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(3.4336, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(3.2827, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(3.2850, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(3.3721, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(3.3297, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(3.2832, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(3.2070, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(3.2097, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(3.1011, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(3.1797, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(3.0183, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.9926, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(3.1934, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(3.0314, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(3.4091, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(3.2364, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(3.3181, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(3.0775, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(3.1700, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(3.0206, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.7647, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.9200, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(3.0748, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(3.1695, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(3.0829, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.7138, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.9952, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(3.1019, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.9766, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(3.0268, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(3.0678, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.9576, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.9944, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.8586, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.8737, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.9895, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(3.0949, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.7636, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.9994, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.9859, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(3.0448, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.8879, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.8155, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.8349, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.6644, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(3.0440, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.7249, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.9664, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.8149, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.7987, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.7939, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.8903, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.8965, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.8219, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.7508, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.8677, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.8721, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.6171, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.8126, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.8603, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.9507, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.8468, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.9401, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(3.1134, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.6709, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.8380, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.6578, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.6181, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.9264, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.9837, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.5744, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.8094, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.6102, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.6892, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.7015, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(3.0127, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.6278, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.8612, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.7189, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.9268, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(3.0501, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.6587, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.7362, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.8292, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.8662, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.7117, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(3.1657, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.9879, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.6075, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.9275, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.7900, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(3.0242, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.7506, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.8359, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.7414, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.7159, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.6748, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.7844, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.7318, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.9643, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.6049, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.8183, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.6853, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.9022, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.7561, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.7132, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.9609, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.9068, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.6877, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.8509, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.8586, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.9369, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.8233, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.7294, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.5407, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.5939, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.5950, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.7476, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.7507, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.9441, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.7563, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.6741, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.5468, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.6394, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.7089, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.8070, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.7616, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.7589, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.8062, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.8234, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.8358, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.4891, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.7058, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.7748, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.6948, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.5912, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.8128, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.4684, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.6086, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.6630, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.7280, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.9052, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.7521, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.6552, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.6777, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.7392, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.8047, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.4687, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.6613, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.6321, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.5232, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.7152, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.5746, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.7830, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.6512, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.7734, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.7136, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.9068, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.6552, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.6776, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.5667, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.6767, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.6239, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.7690, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.4352, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.7140, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.3127, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.6304, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.5033, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.6425, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.8006, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.5642, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.5921, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.9118, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.6860, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.6707, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.7841, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.5916, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.5980, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.8924, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.5485, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.6939, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.8029, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.5110, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.4897, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.6558, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.5723, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.8548, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.7156, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.8066, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.5603, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.6975, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.4066, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.4730, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.4619, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.5297, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.7170, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.8113, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.9000, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.8112, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.6328, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.7196, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.9422, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.5712, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.7344, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.5874, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.5594, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.8396, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.5612, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.6300, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.7051, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.5803, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.6429, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.6738, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.6411, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.7987, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.7905, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.7311, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.5390, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.7047, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.7696, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.5541, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.7303, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.6598, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.5492, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.7709, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.6781, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.6170, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.8842, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.6541, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.8389, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.3896, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.5161, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.5639, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.6582, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.5525, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.6715, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.5670, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.4412, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.6276, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.4467, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.5770, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.6021, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.7216, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.6355, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.5798, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.6744, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.6817, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.8330, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.5122, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.6431, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.5399, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.4832, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.7738, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.4894, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.3507, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.6853, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.5005, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.6380, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.5767, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.4547, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.8479, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.7525, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.6866, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.5855, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.6257, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.8249, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.5664, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.5116, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.6048, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.5328, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.4743, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.5020, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.6214, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.9893, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.6898, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.7221, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.7048, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.4158, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.6250, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.6367, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.6130, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.5164, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.6494, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.7094, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.7205, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.6939, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.5535, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.4847, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.8076, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.6358, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.5717, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.7109, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.8042, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.7373, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.7103, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.6706, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.7898, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.6805, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.5560, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.5365, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.7940, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.7523, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.5030, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.6481, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.7589, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.6270, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.4796, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.5841, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.6182, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.6520, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.5823, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.7645, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.4735, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.6805, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.4923, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.5594, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.5249, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.5938, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.8265, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.6361, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.4778, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.6485, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.5435, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.7047, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.6542, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.4887, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.7571, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.6388, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.5830, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.5568, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.6731, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.5378, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.4879, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.4109, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.3402, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.7214, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.5419, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.7877, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.6654, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.8788, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.7021, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.7395, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.5006, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.5650, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.5710, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.5218, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.7275, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.7365, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.5120, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.5164, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.7419, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.7560, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.4977, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.4914, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.6194, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.6462, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.7314, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.6254, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.7164, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.5627, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.6178, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.7721, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.7247, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.5366, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.6643, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.5663, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.7332, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.6254, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.6832, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.6689, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.6126, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.7222, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.6289, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.5794, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.6318, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.7018, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.5064, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.6284, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.5005, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.5733, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.4416, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.6592, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.7285, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.5212, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.5817, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.6290, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.5754, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.6055, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.6220, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.5311, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.6226, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.7362, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.6655, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.6526, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.7781, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.7003, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.5365, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.6285, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.6037, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.7117, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.5757, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.6319, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.6956, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.5317, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.5814, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.4227, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.3941, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.5812, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.4642, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.5892, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.3956, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.7462, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.6842, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.6410, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.6374, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.3555, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.6202, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.7242, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.7205, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.6959, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.6036, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.4296, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training...\n",
      "Training Loss:  tensor(2.4953, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 49\u001b[0m\n\u001b[0;32m     46\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     47\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m---> 49\u001b[0m     total_train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     53\u001b[0m avg_loss \u001b[38;5;241m=\u001b[39m total_train_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader)\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m], Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "log_dir = 'E:/FYP/Egg-Based Emotion Recognition/EEg-based-Emotion-Recognition/runs/CLISA'\n",
    "writer = SummaryWriter(log_dir)\n",
    "# Initialize Model, Optimizer, and Scheduler\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "base_encoder = BaseEncoder().to(device)\n",
    "projector = Projector().to(device)\n",
    "model = CLISA(base_encoder, projector).to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0007, weight_decay=0.015)\n",
    "scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2)  # T_0: Initial restart period\n",
    "\n",
    "# Early Stopping Parameters\n",
    "early_stopping_tolerance = 30\n",
    "best_val_loss = float('inf')\n",
    "epochs_without_improvement = 0\n",
    "\n",
    "\n",
    "# Training Loop with Early Stopping and Scheduler\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "\n",
    "    for eeg_a, eeg_b in train_loader:\n",
    "        print(\"Training...\")\n",
    "        eeg_a, eeg_b = eeg_a.to(device).float(), eeg_b.to(device).float()\n",
    "\n",
    "        # Forward pass\n",
    "        z_a = model(eeg_a)\n",
    "        z_b = model(eeg_b)\n",
    "\n",
    "        # Compute contrastive loss\n",
    "        loss = contrastive_loss(z_a.view(z_a.size(0), -1), z_b.view(z_b.size(0), -1))\n",
    "        print(\"Training Loss: \", loss)\n",
    "        writer.add_scalar('Loss/train', loss.item(), epoch*len(train_loader)+len(train_loader))\n",
    "        \n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "\n",
    "\n",
    "    avg_loss = total_train_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        save_checkpoint(model, optimizer, epoch, avg_loss, f'./Checkpoints/checkpoint_{epoch}.pt')\n",
    "    \n",
    "    # Log to TensorBoard\n",
    "    writer.add_scalar('AvgLoss/train', avg_loss, epoch)\n",
    "    writer.add_scalar('Learning Rate', scheduler.get_last_lr()[0], epoch)\n",
    "        \n",
    "    # Step the scheduler\n",
    "    scheduler.step()\n",
    "\n",
    "    # Validation Loop\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    print(\"Validating...\")\n",
    "    with torch.no_grad():\n",
    "        for eeg_a, eeg_b in val_loader:\n",
    "            eeg_a, eeg_b = eeg_a.to(device).float(), eeg_b.to(device).float()\n",
    "\n",
    "            z_a = model(eeg_a)\n",
    "            z_b = model(eeg_b)\n",
    "\n",
    "            loss = contrastive_loss(z_a.view(z_a.size(0), -1), z_b.view(z_b.size(0), -1))\n",
    "            print(\"Validation Loss: \", loss)\n",
    "            writer.add_scalar('Loss/val', loss.item(), epoch*len(val_loader)+len(val_loader))\n",
    "            total_val_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "    writer.add_scalar('AvgLoss/val', avg_val_loss, epoch)\n",
    "\n",
    "    # Print epoch details\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    # Early Stopping Check\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        epochs_without_improvement = 0\n",
    "    else:\n",
    "        epochs_without_improvement += 1\n",
    "\n",
    "    if epochs_without_improvement >= early_stopping_tolerance:\n",
    "        print(\"Early stopping triggered.\")\n",
    "        break\n",
    "\n",
    "# Close TensorBoard Writer\n",
    "writer.close()\n",
    "print(\"Contrastive learning completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
