{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-12-08 00:00:08] INFO (torcheeg/MainThread) üîç | Detected cached processing results, reading cache from E:/FYP/Egg-Based Emotion Recognition/EEg-based-Emotion-Recognition/.torcheeg/datasets_1733174610032_5iJyS.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw EEG data shape: (62, 200)\n"
     ]
    }
   ],
   "source": [
    "from torcheeg.datasets import SEEDDataset\n",
    "from torcheeg import transforms\n",
    "\n",
    "raw_dataset = SEEDDataset(\n",
    "    root_path='./SEED/SEED_EEG/Preprocessed_EEG',\n",
    "    io_path = 'E:/FYP/Egg-Based Emotion Recognition/EEg-based-Emotion-Recognition/.torcheeg/datasets_1733174610032_5iJyS',\n",
    "    online_transform=None,  # Disable transforms\n",
    "    label_transform=None,\n",
    "    num_worker=4\n",
    ")\n",
    "\n",
    "raw_sample = raw_dataset[0]\n",
    "print(f\"Raw EEG data shape: {raw_sample[0].shape}\")  # Should be [62, ...] for SEED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.signal import butter, lfilter\n",
    "\n",
    "# Define a bandpass filter (4-47 Hz for SEED dataset)\n",
    "def bandpass_filter(data, lowcut=4, highcut=47, fs=200, order=4):\n",
    "    nyquist = 0.5 * fs\n",
    "    low = lowcut / nyquist\n",
    "    high = highcut / nyquist\n",
    "    b, a = butter(order, [low, high], btype='band')\n",
    "    return lfilter(b, a, data, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset(dataset):\n",
    "    processed_segments = []\n",
    "    for sample in dataset:\n",
    "        eeg_data, metadata = sample\n",
    "\n",
    "        # Bandpass filter\n",
    "        filtered_data = bandpass_filter(eeg_data)\n",
    "\n",
    "        # Append metadata for reference\n",
    "        processed_segments.append((filtered_data, metadata))\n",
    "\n",
    "    return processed_segments\n",
    "# Process the raw dataset\n",
    "processed_data = process_dataset(raw_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def incremental_normalization(process_dataset):\n",
    "    normalized_segments = []\n",
    "    subject_data_stats = {}\n",
    "    \n",
    "    #First pass : computing mean and variance incrementally for each subject\n",
    "    for eeg_data, metadata in process_dataset:\n",
    "        subject_id = metadata['subject_id']\n",
    "        if subject_id not in subject_data_stats:\n",
    "            subject_data_stats[subject_id] = {'sum':0, 'sum_sq':0, \"count\":0}\n",
    "            \n",
    "        stats = subject_data_stats[subject_id]\n",
    "        stats['sum'] += np.sum(eeg_data, axis=-1, keepdims=True)\n",
    "        stats['sum_sq'] += np.sum(eeg_data**2, axis=-1, keepdims=True)\n",
    "        stats['count'] += eeg_data.shape[1]\n",
    "        \n",
    "    #Compute mean and standard deviation for each subject\n",
    "    for subject_id, stats in subject_data_stats.items():\n",
    "        stats['mean'] = stats['sum'] / stats['count']\n",
    "        stats['std'] = np.sqrt(stats['sum_sq'] / stats['count'] - stats['mean']**2)\n",
    "        \n",
    "        \n",
    "    #Second pass : Normalizing each segment using computed stats\n",
    "    for eeg_data, metadata in process_dataset:\n",
    "        subject_id = metadata['subject_id']\n",
    "        stats = subject_data_stats[subject_id]\n",
    "        mean = stats['mean']\n",
    "        std = stats['std']\n",
    "        normalized_data = (eeg_data - mean) / std\n",
    "        normalized_segments.append((normalized_data, metadata))\n",
    "        \n",
    "    return normalized_segments\n",
    "\n",
    "# Normalize the processed dataset\n",
    "normalized_data = incremental_normalization(processed_data)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to sample data into fixed length segments\n",
    "def sample_data(normalized_data, time_length=30, step_size=15):\n",
    "    sampled_segments = []\n",
    "    \n",
    "    for eeg_data, metadata in normalized_data:\n",
    "        trial_length = eeg_data.shape[1]\n",
    "        \n",
    "        for start in range(0 , trial_length - time_length + 1, step_size):\n",
    "            segment = eeg_data[:, start:start+time_length]\n",
    "            new_metadata = metadata.copy()\n",
    "            new_metadata['segment_start'] = start\n",
    "            new_metadata['segment_end'] = start + time_length\n",
    "            sampled_segments.append((segment, new_metadata))\n",
    "    \n",
    "    return sampled_segments\n",
    "\n",
    "# Sample the normalized data\n",
    "time_length = 30\n",
    "step_size = 15\n",
    "sampled_data = sample_data(normalized_data, time_length=time_length, step_size=step_size)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute DE features for each segment\n",
    "def compute_de_features(eeg_segments):\n",
    "    # eeg_segment shape: [channels, timepoints]\n",
    "    variance = np.var(eeg_segments, axis=-1)\n",
    "    de_features = 0.5 * np.log2(2 * np.pi * np.e * variance)\n",
    "    return de_features\n",
    "\n",
    "# Extract DE features for the sampled data\n",
    "de_features_data = []\n",
    "for segment, metadata in sampled_data:\n",
    "    de_features = compute_de_features(segment)\n",
    "    de_features_data.append((de_features, metadata))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class BaseEncoder(nn.Module):\n",
    "    def __init__(self, input_channels=62, temporal_filter_length=48, spatial_filters=16, temporal_filters=16):\n",
    "        super(BaseEncoder, self).__init__()\n",
    "        # Spatial Convolution\n",
    "        self.spatial_conv = nn.Conv1d(input_channels, spatial_filters, kernel_size=1)\n",
    "        # Temporal Convolution\n",
    "        self.temporal_conv = nn.Conv1d(spatial_filters, temporal_filters, kernel_size=temporal_filter_length, padding=temporal_filter_length // 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: [batch_size, input_channels, time_points]\n",
    "        print(\"Entered BaseEncoder\")\n",
    "        x = self.spatial_conv(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        x = self.temporal_conv(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        print(\"Exit BaseEncoder\")\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Projector\n",
    "class Projector(nn.Module):\n",
    "    def __init__(self, spatial_filters=16, pooling_kernel=24, temporal_filter_size=4 , c=2):\n",
    "        super(Projector, self).__init__()\n",
    "        #Average Pooling\n",
    "        \n",
    "        self.avg_pool = nn.AvgPool1d(kernel_size=pooling_kernel , stride=pooling_kernel)\n",
    "        #spatial convolution        \n",
    "        self.spatial_conv = nn.Conv1d(in_channels=spatial_filters, out_channels=spatial_filters*c,kernel_size=1)\n",
    "        #temporal convolution\n",
    "        self.temporal_conv = nn.Conv1d(in_channels=spatial_filters*c,out_channels=(spatial_filters*c)*c, kernel_size=temporal_filter_size, padding=temporal_filter_size//2)\n",
    "        #Activation\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        print(\"Entered Projector\")\n",
    "        #Applying avg pooling\n",
    "        x = self.avg_pool(x)\n",
    "        #Applying spatial convolution\n",
    "        x = self.spatial_conv(x)\n",
    "        x = self.relu(x)\n",
    "        #Applying temporal convolution\n",
    "        x = self.temporal_conv(x)\n",
    "        x = self.relu(x)\n",
    "        print(\"Exit Projector\")\n",
    "        return x\n",
    "    \n",
    "\n",
    "\n",
    "# Initialize the projector\n",
    "projector = Projector(spatial_filters=16, pooling_kernel=24, temporal_filter_size=4,c=2)\n",
    "\n",
    "# # Test on the encoded output\n",
    "# projected_output = projector(encoded_output)\n",
    "\n",
    "# print(f\"Encoded output shape: {encoded_output.shape}\")\n",
    "# print(f\"Projected output shape: {projected_output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self, temperature=0.2):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def forward(self, z_i, z_j):\n",
    "        print(\"Entered ContrastiveLoss\")\n",
    "\n",
    "        # Normalize embeddings\n",
    "        z_i = nn.functional.normalize(z_i, dim=1)\n",
    "        z_j = nn.functional.normalize(z_j, dim=1)\n",
    "\n",
    "        # Compute similarity matrix\n",
    "        similarity_matrix = torch.matmul(z_i, z_j.T) / self.temperature\n",
    "\n",
    "        # Clamp similarities to prevent overflow\n",
    "        similarity_matrix = torch.clamp(similarity_matrix, min=-10, max=10)\n",
    "\n",
    "        # Debugging similarities and labels\n",
    "        print(f\"Similarity Matrix: {similarity_matrix.shape}\")\n",
    "        print(f\"Similarity Matrix (First 5 Rows & Columns):\\n{similarity_matrix[:5, :5]}\")\n",
    "\n",
    "        # Create labels for the batch\n",
    "        batch_size = z_i.size(0)\n",
    "        labels = torch.arange(batch_size).to(z_i.device)\n",
    "        print(f\"Labels: {labels}\")\n",
    "\n",
    "        # Compute contrastive loss\n",
    "        loss = nn.CrossEntropyLoss()(similarity_matrix, labels)\n",
    "        print(f\"Contrastive Loss: {loss.item()}\")\n",
    "\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Subject IDs: [9, 10, 11, 12, 13, 14, 0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "Subject Sample Counts: Counter({9: 122184, 10: 122184, 11: 122184, 12: 122184, 13: 122184, 14: 122184, 0: 122184, 1: 122184, 2: 122184, 3: 122184, 4: 122184, 5: 122184, 6: 122184, 7: 122184, 8: 122184})\n",
      "Updated Subject IDs: [9, 10, 11, 12, 13, 14, 0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "Updated Subject Sample Counts: Counter({9: 122184, 10: 122184, 11: 122184, 12: 122184, 13: 122184, 14: 122184, 0: 122184, 1: 122184, 2: 122184, 3: 122184, 4: 122184, 5: 122184, 6: 122184, 7: 122184, 8: 122184})\n"
     ]
    }
   ],
   "source": [
    "# Check unique subject IDs and their sample counts\n",
    "from collections import Counter\n",
    "\n",
    "subject_ids = [item[1]['subject_id'] for item in sampled_data]\n",
    "subject_distribution = Counter(subject_ids)\n",
    "\n",
    "print(f\"Unique Subject IDs: {list(subject_distribution.keys())}\")\n",
    "print(f\"Subject Sample Counts: {subject_distribution}\")\n",
    "\n",
    "# Create a mapping for subject IDs\n",
    "unique_subject_ids = sorted(set(subject_ids))\n",
    "subject_id_map = {old_id: new_id for new_id, old_id in enumerate(unique_subject_ids)}\n",
    "\n",
    "# Update subject IDs in metadata\n",
    "for _, metadata in sampled_data:\n",
    "    metadata['subject_id'] = subject_id_map[metadata['subject_id']]\n",
    "\n",
    "# Verify remapping\n",
    "updated_subject_ids = [item[1]['subject_id'] for item in sampled_data]\n",
    "updated_subject_distribution = Counter(updated_subject_ids)\n",
    "\n",
    "print(f\"Updated Subject IDs: {list(updated_subject_distribution.keys())}\")\n",
    "print(f\"Updated Subject Sample Counts: {updated_subject_distribution}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 1710576, Testing samples: 122184\n"
     ]
    }
   ],
   "source": [
    "def loso_split(dataset, num_subjects):\n",
    "    \"\"\"\n",
    "    Create training and testing splits for Leave-One-Subject-Out (LOSO) Cross-Validation.\n",
    "    Args:\n",
    "        dataset: list of tuples (EEG_data, metadata)\n",
    "        num_subjects: total number of unique subjects in the dataset\n",
    "    Returns:\n",
    "        List of splits [(train_data, test_data), ...]\n",
    "    \"\"\"\n",
    "    splits = []\n",
    "    for test_subject in range(num_subjects):\n",
    "        # Split dataset into training and testing based on subject ID\n",
    "        train_data = [item for item in dataset if item[1]['subject_id'] != test_subject]\n",
    "        test_data = [item for item in dataset if item[1]['subject_id'] == test_subject]\n",
    "        splits.append((train_data, test_data))\n",
    "    return splits\n",
    "\n",
    "# Determine the number of unique subjects\n",
    "num_subjects = len(set(item[1]['subject_id'] for item in sampled_data))\n",
    "\n",
    "# Perform LOSO split\n",
    "loso_splits = loso_split(sampled_data, num_subjects)\n",
    "\n",
    "# Example: Use the first split\n",
    "train_data, test_data = loso_splits[0]\n",
    "print(f\"Training samples: {len(train_data)}, Testing samples: {len(test_data)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class EEGDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # print(\"Entered EEGDataset\")\n",
    "        eeg_data, metadata = self.data[idx]\n",
    "        if not isinstance(metadata, dict):\n",
    "            raise ValueError(f\"Metadata must be a dictionary, but got {type(metadata)} at index {idx}.\")\n",
    "        eeg_data_tensor = torch.tensor(eeg_data, dtype=torch.float32)\n",
    "        emotion_label = metadata.get('emotion', -1) # Replace 'emotion' with the actual key for emotion labels\n",
    "        # print(\"Exit EEGDataset\")\n",
    "        return eeg_data_tensor, metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratified_normalization_minibatch(x, metadata):\n",
    "    \"\"\"\n",
    "    Perform stratified normalization of the minibatch based on subject and emotion information.\n",
    "    x: Tensor of shape [batch_size, channels, time_points].\n",
    "    metadata: Dictionary of metadata with keys like 'subject_id', 'emotion', etc.\n",
    "    \"\"\"\n",
    "    print(\"Entered stratified_normalization_minibatch\")\n",
    "    print(f\"x.shape: {x.shape}\")\n",
    "    print(f\"Metadata keys: {list(metadata.keys())}\")\n",
    "\n",
    "    # Extract subject_id and emotion as tensors\n",
    "    subject_ids = metadata['subject_id']\n",
    "    emotion_labels = metadata['emotion']\n",
    "\n",
    "    # Initialize normalized tensor\n",
    "    normalized_x = torch.zeros_like(x)\n",
    "\n",
    "    # Group data by (subject_id, emotion)\n",
    "    group_to_indices = {}\n",
    "    for idx, (subject, emotion) in enumerate(zip(subject_ids, emotion_labels)):\n",
    "        group = (subject.item(), emotion.item())\n",
    "        if group not in group_to_indices:\n",
    "            group_to_indices[group] = []\n",
    "        group_to_indices[group].append(idx)\n",
    "\n",
    "    for group, indices in group_to_indices.items():\n",
    "        group_data = x[indices]  # Extract data for the group\n",
    "        mean = group_data.mean(dim=-1, keepdim=True)  # Mean over time points\n",
    "        std = group_data.std(dim=-1, keepdim=True)    # Std over time points\n",
    "        normalized_x[indices] = (group_data - mean) / (std + 1e-8)  # Normalize\n",
    "\n",
    "    print(\"Exit stratified_normalization_minibatch\")\n",
    "    return normalized_x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "def save_checkpoint(epoch, model, optimizer, loss, path=\"./Checkpoints/\"):\n",
    "    # Ensure the directory exists\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    \n",
    "    save_path = os.path.join(path, f\"checkpoint_epoch_{epoch}.pth\")\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss,\n",
    "    }, save_path)\n",
    "    print(f\"Checkpoint saved at: {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(filename, model, optimizer):\n",
    "    \"\"\"\n",
    "    Load a training checkpoint.\n",
    "    Args:\n",
    "        filename: Name of the checkpoint file.\n",
    "        model: The model to load the state into.\n",
    "        optimizer: The optimizer to load the state into.\n",
    "    Returns:\n",
    "        epoch: The epoch at which the checkpoint was saved.\n",
    "        loss: The loss value at the time of saving.\n",
    "    \"\"\"\n",
    "    checkpoint = torch.load(filename)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    epoch = checkpoint['epoch']\n",
    "    loss = checkpoint['loss']\n",
    "    print(f\"Checkpoint loaded: {filename} (epoch {epoch})\")\n",
    "    return epoch, loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def train_model_with_stratified_normalization(train_data, base_encoder, projector, contrastive_loss, epochs=100, batch_size=256, lr=0.0007, patience=30, checkpoint_path=\"./Checkpoints/\"):\n",
    "   \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    writer = SummaryWriter(log_dir=\"E:/FYP/Egg-Based Emotion Recognition/EEg-based-Emotion-Recognition/runs/ContrastiveLearning\")\n",
    "    \n",
    "    checkpoint_dir = \"./Checkpoints/\"\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "    \n",
    "    train_dataset = EEGDataset(train_data)\n",
    "    print(f\"Total samples in Dataset: {len(train_dataset)}\")\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0, timeout=0)\n",
    "    print(f\"Total batches in DataLoader: {len(train_loader)}\")\n",
    "\n",
    "    # Optimizer and scheduler\n",
    "    model_params = list(base_encoder.parameters()) + list(projector.parameters())\n",
    "    optimizer = optim.Adam(model_params, lr=lr, weight_decay=0.015)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2)\n",
    "\n",
    "    base_encoder.train()\n",
    "    projector.train()\n",
    "\n",
    "    best_loss = float(\"inf\")\n",
    "    epochs_no_improve = 0\n",
    "    \n",
    "    start_epoch = 0\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        start_epoch, best_loss = load_checkpoint(checkpoint_path, base_encoder, optimizer)\n",
    "        \n",
    "\n",
    "    for epoch in range(start_epoch,epochs):\n",
    "        print(f\"Entered Epoch {epoch + 1}\")\n",
    "        total_loss = 0.0\n",
    "        print(f\"Starting Training Loop with {len(train_loader)} batches.\")\n",
    "\n",
    "\n",
    "        for batch_idx, (x, metadata) in enumerate(train_loader):\n",
    "            print(f\"Processing Batch {batch_idx + 1}/{len(train_loader)}\")\n",
    "            print(f\"Batch {batch_idx + 1} loaded: x.shape={x.shape}, metadata keys = {list(metadata.keys())}\")\n",
    "           \n",
    "            print(\"Entering try block\")\n",
    "            try:\n",
    "                print(\"Inside try block\")\n",
    "                x = x.float().to(device)  # Send input to device\n",
    "              \n",
    "                # Apply stratified normalization on the minibatch\n",
    "                x = stratified_normalization_minibatch(x, metadata)\n",
    "                print(f\"Normalized Input Shape: {x.shape}\")\n",
    "\n",
    "                # Forward pass through Base Encoder and Projector\n",
    "                encoded = base_encoder(x)\n",
    "                print(f\"Encoded Output Shape: {encoded.shape}\")\n",
    "                \n",
    "                z_i = projector(encoded)\n",
    "                print(f\"Projected z_i Shape: {z_i.shape}\")\n",
    "                \n",
    "                z_j = projector(encoded + torch.normal(mean=0, std=0.005, size=encoded.shape).to(device))  \n",
    "                print(f\"Projected z_j Shape: {z_j.shape}\")\n",
    "                \n",
    "                z_i = z_i.view(z_i.size(0), -1)\n",
    "                z_j = z_j.view(z_j.size(0), -1)\n",
    "                z_i = z_i.to(device)\n",
    "                z_j = z_j.to(device)\n",
    "\n",
    "                # Compute Contrastive Loss\n",
    "                loss = contrastive_loss(z_i,z_j)  # Replace second projected with positive pairs if applicable\n",
    "                print(f\"Batch {batch_idx + 1}, Loss: {loss.item()}\")\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                writer.add_scalar(\"Batch Loss\", loss.item(), epoch * len(train_loader) + batch_idx)\n",
    "\n",
    "                # Backpropagation and optimization step\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error in Batch {batch_idx + 1}: {e}\")\n",
    "                break\n",
    "                      \n",
    "\n",
    "        # Average loss over all batches\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        writer.add_scalar(\"Avg Loss\", avg_loss, epoch)\n",
    "        \n",
    "        if epoch % 5 == 0 or avg_loss < best_loss:\n",
    "            save_checkpoint_path = os.path.join(checkpoint_dir, f\"checkpoint_epoch_{epoch}.pth\")\n",
    "            save_checkpoint(epoch, base_encoder, optimizer, avg_loss, save_checkpoint_path)\n",
    "\n",
    "        \n",
    "            \n",
    "        # Early stopping logic\n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "            break\n",
    "\n",
    "        # Step the scheduler\n",
    "        scheduler.step()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    writer.close()\n",
    "    print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Initialize models\n",
    "base_encoder = BaseEncoder(input_channels=62, temporal_filter_length=48, spatial_filters=16, temporal_filters=16).to(device)\n",
    "projector = Projector(spatial_filters=16, pooling_kernel=24, temporal_filter_size=4, c=2).to(device)\n",
    "contrastive_loss = ContrastiveLoss(temperature=0.1).to(device)\n",
    "\n",
    "# Train with stratified normalization\n",
    "train_model_with_stratified_normalization(\n",
    "    train_data, base_encoder, projector, contrastive_loss, epochs=100, batch_size=256, lr=0.0007, patience=30 , checkpoint_path=\"./Checkpoints/check/\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
